# Training Configuration for MNIST Dataset
# This is an example configuration file demonstrating the YAML-based training workflow

dataset:
  name: "mnist"
  data_path: null  # Will use default torchvision download
  batch_size: 64
  num_workers: 4
  shuffle: true
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  seed: 42
  augmentation: false
  augmentation_params: {}

model:
  name: "adaptive"
  input_dim: 784
  hidden_dim: 128
  output_dim: 10
  num_nodes: 64
  dropout: 0.1
  activation: "relu"
  model_params: {}

optimizer:
  name: "adam"
  learning_rate: 0.001
  weight_decay: 0.0001
  momentum: 0.9
  betas: [0.9, 0.999]
  eps: 1.0e-8
  scheduler: "cosine"
  scheduler_params:
    T_max: 10
    eta_min: 1.0e-6

training:
  epochs: 10
  max_steps: null
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  use_amp: false
  checkpoint_dir: "checkpoints/mnist"
  save_every_n_epochs: 1
  keep_last_n_checkpoints: 3
  log_every_n_steps: 10
  log_dir: "logs/mnist"
  verbose: false
  device: "cuda"
  seed: 42
  early_stopping: false
  early_stopping_patience: 5
  early_stopping_metric: "val_loss"

evaluation:
  metrics: ["accuracy", "loss", "f1"]
  batch_size: 128
  save_predictions: false
  output_dir: "outputs/mnist"
