# Quick Training Configuration
# For fast experimentation and debugging

dataset:
  name: "synthetic"
  data_path: null
  batch_size: 32
  num_workers: 2
  shuffle: true
  train_split: 0.8
  val_split: 0.1
  test_split: 0.1
  seed: 42
  augmentation: false
  augmentation_params: {}

model:
  name: "adaptive"
  input_dim: 64
  hidden_dim: 32
  output_dim: 10
  num_nodes: 16
  dropout: 0.1
  activation: "relu"
  model_params: {}

optimizer:
  name: "adam"
  learning_rate: 0.001
  weight_decay: 0.0
  momentum: 0.9
  betas: [0.9, 0.999]
  eps: 1.0e-8
  scheduler: null
  scheduler_params: {}

training:
  epochs: 3
  max_steps: null
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  use_amp: false
  checkpoint_dir: "checkpoints/quick"
  save_every_n_epochs: 1
  keep_last_n_checkpoints: 1
  log_every_n_steps: 5
  log_dir: "logs/quick"
  verbose: true
  device: "cpu"
  seed: 42
  early_stopping: false
  early_stopping_patience: 3
  early_stopping_metric: "val_loss"

evaluation:
  metrics: ["accuracy", "loss"]
  batch_size: 64
  save_predictions: false
  output_dir: "outputs/quick"
