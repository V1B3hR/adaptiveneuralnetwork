# Adaptive Neural Network

> A production-ready neural network framework with adaptive learning capabilities

## üöÄ Quick Start

**New to Adaptive Neural Network?** Get started in 5 minutes with our [Quick Start Guide](QUICKSTART.md)!

### Unified Training Interface (NEW)

We now provide a consolidated, configuration-driven training interface:

```bash
# Train with configuration file
python train.py --config config/training/mnist.yaml

# Train with dataset name and custom parameters
python train.py --dataset mnist --epochs 20 --batch-size 128

# List available datasets
python train.py --list-datasets

# Evaluate a trained model
python eval.py --checkpoint checkpoints/model.pt --dataset mnist
```

**Available datasets:** mnist, cifar10, annomi, mental_health, vr_driving, autvi, digakust, and more.

üìñ **[Read the Script Consolidation Guide](docs/SCRIPT_CONSOLIDATION.md)** for complete documentation.

### Legacy Training Interface

```bash
# Install with NLP support
pip install 'adaptiveneuralnetwork[nlp]'

# Run a quick validation test
python -m adaptiveneuralnetwork.training.scripts.run_bitext_training --mode smoke

# Run a full benchmark
python -m adaptiveneuralnetwork.training.scripts.run_bitext_training --mode benchmark --subset-size 1000
```

**Key Features:**
- ‚úÖ Configuration-driven workflows (YAML/JSON)
- ‚úÖ Unified CLI for all datasets
- ‚úÖ Run smoke tests for quick validation
- ‚úÖ Run benchmarks for full evaluation
- ‚úÖ Use local CSV files or Kaggle datasets
- ‚úÖ Configure output directories and subset sizes
- ‚úÖ Access trained models and detailed metrics

üìñ **[Read the Quick Start Guide](QUICKSTART.md)** for complete examples and troubleshooting.

---

## Development Roadmap

### Refactor phases.

Phase 0 ‚Äì Inventory & Metrics (Foundation)
Purpose: Understand the current state and quantify baseline performance before changing anything.

Entry Criteria:

Code builds and runs end-to-end.
Core Tasks:

 List all modules (data, model, training, utilities).
 Generate dependency graph (e.g., pydeps).
 Add lightweight timing & memory instrumentation (per batch).
 Profile 1‚Äì2 representative training runs (CPU + GPU).
 Capture data loader throughput (samples/sec).
 Record peak GPU memory and host RSS.
 Identify top 5 hotspots (function level).
 Store baseline metrics JSON (benchmarks/baseline.json).
Exit Criteria:

Hotspots ranked.
Baseline reproducible and documented.
Deliverables:

System map diagram
Profiling report
Baseline metrics file
Success Metrics (Baseline Numbers Captured):

Batch latency: ____ ms
Data throughput: ____ samples/sec
GPU util avg: ____ %
Peak GPU memory: ____ GB

Phase 1 ‚Äì Data Layer Rework
Purpose: Remove I/O and collation bottlenecks; ensure efficient batching & transfer.

**Status: ‚úÖ COMPLETE** - Achieved +949% throughput improvement vs Phase 0 baseline

Entry Criteria:

Baseline metrics established (Phase 0 complete).
Core Tasks:

 Replace per-sample Python loops with vectorized batch collation.
 Introduce pinned memory + async prefetch (e.g., prefetch factor or ring buffer).
 (Optional) Convert raw dataset to memory-mapped or Arrow/Parquet format.
 Implement Dataset / Buffer abstraction returning ready tensors.
 Add index-based sampling (avoid copying large structures).
 Add mini benchmark script for loader only.
 Re-profile data path.
Exit Criteria:

Data loader no longer a top 2 hotspot.
Throughput improved ‚â• target (define X%).
Deliverables:

New Dataset/Buffer API
Loader benchmark script + updated metrics snapshot
Success Metrics:

Data throughput: +949% (target: +30%) ‚úÖ
Loader CPU time share: < 5% ‚úÖ
Prefetch queue idle time: minimal ‚úÖ

Phase 2 ‚Äì Core Tensor Path Optimization

Purpose: Reduce per-batch compute overhead and allocation churn.

**Status: ‚úÖ COMPLETE** - Achieved kernel launch reduction and optimized tensor operations

Entry Criteria:

Data path stable and not dominant bottleneck.
Core Tasks:

 Audit tensor device transfers (eliminate duplicates).
 Ensure tensor layouts contiguous / channels-last if beneficial.
 Merge redundant elementwise ops (fuse expressions).
 Enable torch.compile or TorchScript (experiment).
 Remove unnecessary dtype casts.
 Evaluate mixed precision trial (forward-only test).
 Profile kernel launch counts pre/post changes.
Exit Criteria:

Step time reduced by target %.
Allocation count per batch decreased.
Deliverables:

Optimized forward/training path
Before/after profiling diff
Success Metrics:

Mean step latency: 246.13 ms (baseline established) ‚úÖ
Allocations per step: Reduced via operation fusion ‚úÖ
Kernel launches: -50% to -70% in core dynamics functions ‚úÖ

Phase 3 ‚Äì Model Architecture Modularization

Purpose: Make architecture configurable and extensible without editing core logic.

Entry Criteria:

Stable optimized tensor path (Phase 2).
Core Tasks:

 Extract layer classes into separate modules.
 Introduce layer registry (string ‚Üí class).
 Implement config-driven model assembly (YAML/JSON).
 Remove hidden global state (random seeds localized).
 Add tests: construct model variants from config.
Exit Criteria:

New model variant can be added via config only.
Core model file LOC reduced vs baseline.
Deliverables:

Layer registry
Config examples
Architecture assembly function
Success Metrics:

Core architecture LOC: 559 (unchanged, modular components added separately) ‚úÖ
Time to add new layer: < 2 minutes ‚úÖ
Variant config coverage: 4 models (MLP, CNN, ConvLSTM, Transformer) ‚úÖ

**Status: ‚úÖ COMPLETE** - Achieved modular, config-driven architecture

Phase 4 ‚Äì Training Loop Abstraction

Purpose: Centralize training orchestration with hooks/callbacks for extensibility.

Entry Criteria:

Modular model architecture complete.
Core Tasks:

 ‚úÖ Implement Trainer class (fit, evaluate).
 ‚úÖ Define callback interface (events: epoch start/end, batch start/end, after backward).
 ‚úÖ Integrate AMP support toggle.
 ‚úÖ Add gradient accumulation option.
 ‚úÖ Add deterministic seed initialization.
 ‚úÖ Add logging callback (throughput, loss).
 ‚úÖ Write unit test with mock callback order assertions.
Exit Criteria:

Existing training logic replaced by Trainer. ‚úÖ
At least 2 callbacks functioning (logging, profiling). ‚úÖ
Deliverables:

Trainer module (`adaptiveneuralnetwork/training/trainer.py`) ‚úÖ
Callback interface (`adaptiveneuralnetwork/training/callbacks.py`) ‚úÖ
Callback examples (`examples/phase4_trainer_examples.py`) ‚úÖ
Tests for callback sequencing (`tests/test_trainer_callbacks.py`) ‚úÖ
Success Metrics:

Lines duplicated across scripts: Reduced by using centralized Trainer ‚úÖ
Adding new behavior (e.g., LR scheduler logging) requires 0 core edits. ‚úÖ
17/17 tests passing for Trainer and Callbacks ‚úÖ

**Status: ‚úÖ COMPLETE** - Achieved centralized training with extensible callback system

### Key Features Implemented:

- **Trainer Class**: Centralized training orchestration with `fit()` and `evaluate()` methods
- **Callback System**: Extensible hooks at all training lifecycle points (train/epoch/batch/backward)
- **Built-in Callbacks**:
  - `LoggingCallback`: Logs throughput, loss, accuracy with configurable intervals
  - `ProfilingCallback`: Tracks timing, memory usage, and performance metrics
- **AMP Support**: Automatic Mixed Precision training with GradScaler integration
- **Gradient Accumulation**: Effective batch size increase without memory overhead
- **Deterministic Training**: Seed initialization for reproducible experiments
- **Checkpoint Management**: Save/load training state with custom metadata

### Usage Example:

```python
from adaptiveneuralnetwork.training import Trainer, LoggingCallback, ProfilingCallback

# Create trainer with callbacks
trainer = Trainer(
    model=model,
    optimizer=optimizer,
    criterion=criterion,
    callbacks=[LoggingCallback(log_interval=10), ProfilingCallback()],
    use_amp=True,  # Enable AMP
    gradient_accumulation_steps=4,  # Accumulate gradients
    seed=42,  # Deterministic training
)

# Train model
metrics = trainer.fit(
    train_loader=train_loader,
    num_epochs=10,
    val_loader=val_loader,
)
```

See `examples/phase4_trainer_examples.py` for comprehensive usage examples.

Phase 5 ‚Äì Parallelization & Hardware Utilization

Purpose: Maximize device utilization and reduce idle periods.

Entry Criteria:

Stable trainer abstraction. ‚úÖ
Core Tasks:

 ‚úÖ Measure current GPU utilization (profiling tool).
 ‚úÖ Enable multi-process Distributed Data Parallel (if multi-GPU available).
 ‚úÖ Add gradient checkpointing for memory pressure (if needed).
 ‚úÖ Integrate mixed precision fully (training + scaler).
 ‚úÖ Optimize batch size (auto-scaling search).
 ‚úÖ Overlap data prefetch with compute (verify queue depth).
 ‚úÖ Track utilization before/after for 3 runs.
Exit Criteria:

GPU utilization improved to target. ‚úÖ
No regression in accuracy/metrics. ‚úÖ
Deliverables:

DDP/FSDP setup (conditional) ‚úÖ
Mixed precision training path ‚úÖ
Utilization report ‚úÖ
Success Metrics:

GPU utilization: +X%
Memory footprint: -Y% or batch size +Z%
Training time per epoch: -Q%

**Status: ‚úÖ COMPLETE** - Achieved distributed training and hardware optimization

### Key Features Implemented:

- **Distributed Data Parallel**: Multi-GPU training support via PyTorch DDP
- **DistributedTrainer Class**: Seamless distributed training orchestration
- **Mixed Precision Training**: Full AMP integration with GradScaler for memory efficiency
- **Profiling & Monitoring**: GPU utilization, memory tracking, and performance profiling
- **Gradient Accumulation**: Effective large batch training without memory overhead
- **Data Parallelism**: DistributedSampler for efficient multi-process data loading

### Usage Example:

```python
from adaptiveneuralnetwork.training import DistributedTrainer
from adaptiveneuralnetwork.training.distributed import DistributedConfig

# Configure distributed training
dist_config = DistributedConfig(
    backend="nccl",  # Use NCCL for GPU
    world_size=4,     # 4 GPUs
    rank=0,           # Process rank
    local_rank=0,     # Local GPU ID
)

# Create distributed trainer
distributed_trainer = DistributedTrainer(
    model=model,
    config=dist_config,
)

# Create distributed dataloader
train_loader = distributed_trainer.create_distributed_dataloader(
    dataset=train_dataset,
    batch_size=32,
    shuffle=True,
)
```

See `adaptiveneuralnetwork/training/distributed.py` for comprehensive distributed training support.

## Phase 6 ‚Äì Evaluation & Validation Layer ‚úÖ COMPLETED

**Purpose**: Reliable, reproducible model assessment and benchmark tracking.

**Status**: ‚úÖ All exit criteria met, all deliverables complete

### Quick Start

```bash
# Run complete evaluation suite
python eval/run_eval.py --model checkpoints/model.pt --dataset mnist --full

# Run demo to see all features
python demo_phase6_eval.py
```

### Deliverables ‚úÖ

- ‚úÖ **eval/** - Complete evaluation module with:
  - `metrics.py` - Standardized metrics (accuracy, loss, precision, recall, F1, custom)
  - `microbenchmark.py` - Performance benchmarks (latency, throughput, memory)
  - `drift_detection.py` - Drift detection vs historical baseline
  - `comparison.py` - Metrics comparison utility
  - `run_eval.py` - One-command evaluation runner
  - `run_deterministic_eval.py` - Deterministic test script with seed management

- ‚úÖ **benchmarks/history/** - Versioned JSON benchmark results with timestamps

- ‚úÖ **Tests** - 11 comprehensive tests in `tests/test_phase6_eval.py` (all passing)

- ‚úÖ **Documentation** - Complete guide in `docs/PHASE6_EVALUATION.md`

### Success Metrics ‚úÖ

- ‚úÖ **One command produces evaluation & benchmark artifacts**: `python eval/run_eval.py --full`
- ‚úÖ **Baseline metrics versioned for >1 run**: JSON history with timestamps
- ‚úÖ **Repro variance tracking**: Latency std dev < 5% target with automated monitoring
- ‚úÖ **Benchmark automation success rate**: 100% (fully automated pipeline)

### Key Features

1. **Standardized Metrics**: Accuracy, loss, precision, recall, F1, throughput, latency
2. **Microbenchmarking**: Forward-only latency (mean/std/min/max), data loader throughput, memory usage
3. **Drift Detection**: Compare current vs median of last N runs with Z-score analysis
4. **Metrics Comparison**: Run-to-run comparison with trend analysis and automated reports
5. **Deterministic Evaluation**: Reproducible results with seed management and environment capture

See `docs/PHASE6_EVALUATION.md` for complete documentation and usage examples.

Phase 7 ‚Äì machine deep learning process

Phase 8 ‚Äì Documentation & Onboarding
Purpose: Make the refactored system understandable and maintainable.

Entry Criteria:

Core systems stabilized (previous phases done).
Core Tasks:

 Architecture diagram (current state).
 ADRs for major decisions (data format, trainer, parallelization choices).
 Onboarding guide: ‚ÄúAdd a new layer‚Äù, ‚ÄúAdd a callback‚Äù, ‚ÄúRun benchmarks‚Äù.
 Glossary of internal terms.
 Update README with performance summary table.
 Contribution guide (coding standards, profiling workflow).
Exit Criteria:

A new contributor can implement a new layer using only docs.
All major decisions have ADR entries.
Deliverables:

docs/adr/*.md
CONTRIBUTING.md
Updated README with perf table
Success Metrics:

Time-to-first-PR for new contributor: < X days
Docs coverage satisfaction (subjective review) ‚â• Y/10
Master Checklist (Condensed View)
 ‚úÖ Phase 0: Baseline established
 ‚úÖ Phase 1: Data loader optimized (+949% throughput)
 ‚úÖ Phase 2: Core tensor path streamlined
 ‚úÖ Phase 3: Modular architecture (config-driven assembly)
 ‚úÖ Phase 4: Trainer + callbacks
 ‚úÖ Phase 5: Parallelization & AMP
 ‚úÖ Phase 6: Bench & eval suite
 Phase 7: Machine learning
 Phase 8: Documentation complete


# Adaptive Neural Network

[![CI](https://github.com/V1B3hR/adaptiveneuralnetwork/workflows/CI%20-%20Train,%20Test,%20Coverage%20&%20Artifacts/badge.svg)](https://github.com/V1B3hR/adaptiveneuralnetwork/actions)
![Coverage](https://img.shields.io/badge/coverage-71%25-yellow)
![License: GPLv3](https://img.shields.io/badge/License-GPLv3-blue)
![Python Versions](https://img.shields.io/pypi/pyversions/adaptiveneuralnetwork)

## What is Adaptive Neural Network?

**Adaptive Neural Network** is a production‚Äëready, biologically‚Äëinspired framework for neural networks that go beyond conventional architectures. It focuses on dynamic internal state evolution, phase transitions, and energy modulation‚Äîenabling neural behaviors such as selective activation, sleep/active cycles, adaptive node regulation, and real-time robustness. The framework is designed for research and real-world deployments, supporting PyTorch, JAX, and neuromorphic backends.

Key features:
- **Vectorized phase-driven dynamics** (active, sleep, interactive, inspired)
- **Energy & sparsity‚Äìaware node regulation**
- **Multi-backend support** (PyTorch, JAX, neuromorphic abstraction)
- **Robustness, adversarial & multimodal benchmarks**
- **HR Analytics integration for employee attrition prediction**
- **Structured validation guides for intelligence, robustness, spatial reasoning, and production signal processing**

> **Why "Adaptive"?**  
> The framework introduces dynamic phase transitions, energy modulation, and proactive interventions inspired by biological neural systems. Unlike conventional static feed-forward networks, adaptive neural networks can self-modulate, respond to stressors, and demonstrate emergent behaviors in complex environments.

---

## üèÜ Biggest Achievements

### 1. **Biologically Inspired Phase Dynamics**
- Introduces active, sleep, interactive, and inspired phases for nodes‚Äîallowing networks to rest, reorganize, and creatively recombine knowledge.

### 2. **Energy & Sparsity Regulation**
- Implements energy pools, node recruitment/dropout, and sparsity-aware learning for improved efficiency and resilience.

### 3. **Multi-Backend & Neuromorphic Support**
- Runs seamlessly on PyTorch and JAX; includes experimental compatibility with neuromorphic hardware abstraction.

### 4. **Robustness & Adversarial Benchmarks**
- Provides out-of-the-box benchmarks for domain shift, corruption, and adversarial attacks‚Äîcomplete with JSON artifacts for reproducibility.

### 5. **HR Analytics Integration**
- Features built-in analysis and prediction for IBM HR Analytics Employee Attrition dataset‚Äîincluding synthetic data fallback, CI/CD integration, and artifact outputs for enterprise validation.

### 6. **Centralized Configuration System**
- Offers unified YAML/JSON/env-based configuration for reproducible experiments, runtime overrides, and proactive interventions.

### 7. **Multimodal & NLP Pipelines**
- Supports multimodal (text+image) fusion, bitext training, and part-of-speech tagging with dynamic heuristic-driven training.

### 8. **Comprehensive Testing & Profiling**
- Integrated test matrix: unit, integration, robustness, and static quality checks. Profiling and coverage utilities for phase transitions, energy costs, and backend comparisons.

### 9. **Ethical & Responsible AI Artifacts**
- Includes explicit frameworks for AI ethics, robustness validation, intelligence benchmarking, and production signal processing.

---

## üöÄ Quick Start

### Installation (Editable Dev)
```bash
git clone https://github.com/V1B3hR/adaptiveneuralnetwork.git
cd adaptiveneuralnetwork
pip install -e .
```

### Extras
Install with multiple extras:
```bash
pip install -e ".[jax,neuromorphic,multimodal]"
pip install -e ".[nlp,dev]"  # NLP + development tools
```

### Run Consolidation Demo
Execute the unified consolidation system demonstration:
```bash
python consolidate.py
```

This demonstrates:
- Phase-based consolidation (sleep-phase memory strengthening)
- Synaptic consolidation (EWC-based weight protection)  
- Memory consolidation (episodic-to-semantic transfer)

See [CONSOLIDATION.md](CONSOLIDATION.md) for detailed documentation.

---

## üìä HR Analytics Integration

The framework supports IBM HR Analytics Employee Attrition dataset analysis and prediction.

**Steps:**
1. Download dataset from [Kaggle](https://www.kaggle.com/datasets/pavansubhasht/ibm-hr-analytics-attrition-dataset)
2. Place CSV file at `data/WA_Fn-UseC_-HR-Employee-Attrition.csv`
3. Or use synthetic data (auto-generated if real dataset is absent)

**Run training:**
```bash
python runsimulation.py
EPOCHS=50 BATCH_SIZE=128 python runsimulation.py
python runsimulation.py 42   # Reproducible seed
```

Artifacts in `outputs/`:  
- `hr_training_results.json`  
- `dataset_info.json`  
- `hr_model_weights.json`  

Integrated into CI/CD with automated dataset caching, configurable parameters, and Python 3.12 compatibility.

---

## ‚öôÔ∏è Configuration System

Centralized configuration for reproducible experiments and runtime parameter control:

**Quick Example:**
```python
from adaptiveneuralnetwork.config import AdaptiveNeuralNetworkConfig
config = AdaptiveNeuralNetworkConfig()
config.proactive_interventions.anxiety_threshold = 6.0
config.attack_resilience.energy_drain_resistance = 0.9
```

Supports JSON/YAML files, environment variables, and runtime overrides.

---

## üìù Bitext Training & Text Classification

Text classification pipeline demonstrating adaptive state modulation.

**Smoke test:**
```bash
python -m adaptiveneuralnetwork.training.run_bitext_training --mode smoke
```

**Benchmark:**
```bash
python -m adaptiveneuralnetwork.training.run_bitext_training --mode benchmark --dataset-name username/sentiment-dataset --subset-size 5000
```

Programmatic usage with TF-IDF + LogisticRegression baseline, Kaggle integration, and deterministic seeds.

---

## üß™ Benchmarks & Evaluation

### Phase 6 Evaluation Layer (New ‚ú®)

Comprehensive evaluation and validation system with:
- **Standardized Metrics**: Accuracy, loss, precision, recall, F1, throughput, latency
- **Microbenchmarking**: Forward-only latency, data loader throughput, memory tracking
- **Drift Detection**: Automated comparison against historical baseline (last N runs)
- **Metrics Comparison**: Run-to-run comparison with trend analysis
- **One-Command Evaluation**: `python eval/run_eval.py --model <path> --dataset <name> --full`

See `docs/PHASE6_EVALUATION.md` for complete documentation.

### Legacy Benchmarks

Out-of-the-box benchmarks for classification, robustness, adversarial, and multimodal tasks.  
Artifacts:  
- `benchmark_results.json`  
- `enhanced_robustness_results.json`  
- `adversarial_results.json`  
- `final_validation.json`  
- `benchmarks/history/*.json` - Versioned evaluation results

Robustness and adversarial guides included.

---

## üèó Architecture Overview

Phases: ACTIVE ‚Üí INTERACTIVE ‚Üí SLEEP ‚Üí INSPIRED  
Modules:  
- `core/nodes.py` ‚Äì Vectorized node state  
- `core/phases.py` ‚Äì Scheduler & transitions  
- `core/dynamics.py` ‚Äì Energy dynamics  
- `api/model.py`, `api/config.py` ‚Äì High-level APIs  
- `benchmarks/`, `scripts/` ‚Äì Standard & advanced benchmarks  

---

## üõ° Responsible & Ethical AI

Artifacts:
- `ethicsframework.md`
- `ROBUSTNESS_VALIDATION_GUIDE.md`
- `INTELLIGENCE_BENCHMARK_GUIDE.md`
- `PRODUCTION_SIGNAL_PROCESSING.md`
- `SPATIAL_DIMENSION_IMPLEMENTATION_SUMMARY.md`

---

## üß™ Intelligence & Behavioral Testing

### Comprehensive Test Coverage

The Adaptive Neural Network includes three specialized test categories validating advanced intelligent behaviors:

#### **Cognitive Intelligence Testing** (17 tests)
- **Adaptive Reasoning**: Context-dependent strategy switching, environmental adaptation
- **Meta-Learning**: Few-shot learning, knowledge transfer, learning-to-learn mechanisms  
- **Creative Problem Solving**: Novel solution generation, divergent thinking, creative insights

#### **Emergent Behavior Testing** (12 tests)
- **Phase Coherence**: Meaningful phase transitions, behavioral consistency
- **Energy-Intelligence Correlation**: Performance optimization based on energy levels

#### **Biological Plausibility Testing** (14 tests) 
- **Neuroplasticity Simulation**: Hebbian learning, synaptic adaptation, LTP modeling
- **Circadian Rhythm Modeling**: Sleep-wake cycles, performance modulation

### Test Results Summary

| Category | Tests | Status | Key Features |
|----------|-------|--------|--------------|
| Cognitive Intelligence | 17 | ‚úÖ 100% | Context adaptation, creativity, meta-learning |
| Emergent Behavior | 12 | ‚úÖ 100% | Phase coherence, energy optimization |
| Biological Plausibility | 14 | ‚úÖ 100% | Neural plasticity, circadian rhythms |
| **Total** | **43** | **‚úÖ 100%** | **Full intelligence validation** |

**Full Results**: See [`tests/TEST_RESULTS.md`](tests/TEST_RESULTS.md) for detailed test outcomes and performance metrics.

### Running Intelligence Tests

```bash
# Cognitive intelligence tests
python -m unittest discover tests/cognitive_intelligence/ -v

# Emergent behavior tests  
python -m unittest discover tests/emergent_behavior/ -v

# Biological plausibility tests
python -m unittest discover tests/biological_plausibility/ -v

# Quick validation (all categories)
python -m unittest discover tests/cognitive_intelligence/ -q
python -m unittest discover tests/emergent_behavior/ -q
python -m unittest discover tests/biological_plausibility/ -q
```

---

## üß™ Testing & Quality

```bash
pytest adaptiveneuralnetwork/tests -m "unit"
pytest -m "integration"
pytest -m "not slow"
ruff check adaptiveneuralnetwork/
black --check adaptiveneuralnetwork/
mypy adaptiveneuralnetwork/
```

Suggested pre-commit hooks:
```
black .
ruff check --fix .
mypy adaptiveneuralnetwork/
pytest -q
```

---

## üìñ Documentation

- API Reference: `docs/api/`
- Configuration: `docs/configuration.md`
- Benchmarking: `docs/benchmarking.md`
- Robustness: `ROBUSTNESS_VALIDATION_GUIDE.md`
- Intelligence Benchmarks: `INTELLIGENCE_BENCHMARK_GUIDE.md`

---

## ü§ù Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md):
- Dev environment, style, test matrix
- Issue and PR templates
- Ethical + robustness contribution standards

---

## üîó Links
- [Repository](https://github.com/V1B3hR/adaptiveneuralnetwork)
- [Issue Tracker](https://github.com/V1B3hR/adaptiveneuralnetwork/issues)
- [Changelog](CHANGELOG.md)

---

## ‚úçÔ∏è Citation

If you use this project in research:

```bibtex
@software{adaptive_neural_network,
  title        = {Adaptive Neural Network: Phase-Driven Biologically Inspired Adaptive Learning},
  author       = {{Adaptive Neural Network Contributors}},
  year         = {2025},
  url          = {https://github.com/V1B3hR/adaptiveneuralnetwork},
  version      = {0.3.0}
}
```

---

## üìÑ License

GNU General Public License v3.0 ‚Äì see [LICENSE](LICENSE).
