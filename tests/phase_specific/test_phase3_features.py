#!/usr/bin/env python3
"""
Test script for Phase 3 advanced intelligence and multimodal learning features.
"""

import time

import torch

from adaptiveneuralnetwork.applications.advanced_language_understanding import (
    AdvancedLanguageConfig,
    AdvancedLanguageUnderstanding,
    LanguageTask,
)
from adaptiveneuralnetwork.applications.iot_edge_integration import (
    SensorData,
    SensorType,
    create_mobile_deployment,
    create_raspberry_pi_deployment,
)
from adaptiveneuralnetwork.applications.multimodal_vl import (
    VisionLanguageConfig,
    VisionLanguageModel,
    VisionLanguageTask,
)
from adaptiveneuralnetwork.models.video_models import (
    VideoModelConfig,
    VideoTextAudioFusion,
    create_advanced_video_transformer,
)


def test_advanced_video_processing():
    """Test advanced video processing with temporal reasoning and action recognition."""
    print("\nüé• Testing Advanced Video Processing...")

    # Create video model configuration
    config = VideoModelConfig(
        input_channels=3,
        input_height=224,
        input_width=224,
        sequence_length=16,
        hidden_dim=256,
        num_classes=400  # Kinetics-400
    )

    # Create advanced video transformer
    model = create_advanced_video_transformer(num_classes=400)
    model.eval()

    # Test with dummy video data
    batch_size = 2
    video_input = torch.randn(batch_size, 16, 3, 224, 224)  # (B, T, C, H, W)

    print(f"   Input video shape: {video_input.shape}")

    with torch.no_grad():
        # Test basic classification
        basic_output = model(video_input, return_detailed=False)
        print(f"   Basic classification output shape: {basic_output.shape}")

        # Test detailed analysis
        detailed_output = model(video_input, return_detailed=True)

        print(f"   ‚úì Classification logits: {detailed_output['classification_logits'].shape}")
        print(f"   ‚úì Current action recognition: {detailed_output['action_recognition']['current_action_logits'].shape}")
        print(f"   ‚úì Future action prediction: {detailed_output['action_recognition']['next_action_logits'].shape}")
        print(f"   ‚úì Action confidence: {detailed_output['action_recognition']['action_confidence'].shape}")
        print(f"   ‚úì Temporal reasoning features: {detailed_output['temporal_reasoning']['temporal_features'].shape}")

        # Display some predictions
        current_actions = torch.softmax(detailed_output['action_recognition']['current_action_logits'], dim=-1)
        future_actions = torch.softmax(detailed_output['action_recognition']['next_action_logits'], dim=-1)
        confidences = detailed_output['action_recognition']['action_confidence']

        for i in range(batch_size):
            top_current = torch.topk(current_actions[i], 3)
            top_future = torch.topk(future_actions[i], 3)

            print(f"   Sample {i}: Current action confidence: {confidences[i].item():.3f}")
            print(f"   Sample {i}: Top current actions: {top_current.indices.tolist()}")
            print(f"   Sample {i}: Top future actions: {top_future.indices.tolist()}")


def test_video_text_audio_fusion():
    """Test multimodal video-text-audio fusion."""
    print("\nüåê Testing Video-Text-Audio Fusion...")

    # Create fusion module
    fusion = VideoTextAudioFusion(
        video_dim=512,
        text_dim=768,
        audio_dim=256,
        fusion_dim=512
    )
    fusion.eval()

    # Test data
    batch_size = 2
    video_features = torch.randn(batch_size, 16, 512)  # Video sequence
    text_features = torch.randn(batch_size, 20, 768)   # Text tokens
    audio_features = torch.randn(batch_size, 32, 256)  # Audio frames

    print(f"   Video features shape: {video_features.shape}")
    print(f"   Text features shape: {text_features.shape}")
    print(f"   Audio features shape: {audio_features.shape}")

    with torch.no_grad():
        fusion_result = fusion(video_features, text_features, audio_features)

        print(f"   ‚úì Fused features: {fusion_result['fused_features'].shape}")
        print(f"   ‚úì Video-text attention: {fusion_result['video_text_attention'].shape}")
        print(f"   ‚úì Video-audio attention: {fusion_result['video_audio_attention'].shape}")
        print(f"   ‚úì Text-audio attention: {fusion_result['text_audio_attention'].shape}")


def test_advanced_language_understanding():
    """Test advanced language understanding capabilities."""
    print("\nüó£Ô∏è Testing Advanced Language Understanding...")

    # Create language understanding configuration
    config = AdvancedLanguageConfig(
        vocab_size=1000,  # Smaller for testing
        embedding_dim=256,
        hidden_dim=256,
        num_layers=4,
        max_sequence_length=128
    )

    # Create language understanding system
    language_system = AdvancedLanguageUnderstanding(config)
    language_system.eval()

    # Test data
    batch_size = 2
    sequence_length = 20
    input_ids = torch.randint(0, config.vocab_size, (batch_size, sequence_length))

    print(f"   Input sequence shape: {input_ids.shape}")

    # Test different language tasks
    tasks_to_test = [
        LanguageTask.POS_TAGGING,
        LanguageTask.SEMANTIC_ROLE_LABELING,
        LanguageTask.DEPENDENCY_PARSING,
    ]

    for task in tasks_to_test:
        print(f"\n   Testing {task.value}...")

        with torch.no_grad():
            if task == LanguageTask.POS_TAGGING:
                # Test POS tagging with character features
                char_ids = torch.randint(0, 128, (batch_size, sequence_length, 10))
                result = language_system(input_ids, task, char_ids=char_ids)

                print(f"     ‚úì POS logits: {result['pos_logits'].shape}")
                print(f"     ‚úì POS predictions: {result['pos_predictions'].shape}")
                print(f"     ‚úì Contextual embeddings: {result['contextual_embeddings'].shape}")

            elif task == LanguageTask.SEMANTIC_ROLE_LABELING:
                result = language_system(input_ids, task)

                print(f"     ‚úì Predicate logits: {result['predicate_logits'].shape}")
                print(f"     ‚úì Argument logits: {result['argument_logits'].shape}")
                print(f"     ‚úì Attention weights: {result['attention_weights'].shape}")

            elif task == LanguageTask.DEPENDENCY_PARSING:
                result = language_system(input_ids, task)

                print(f"     ‚úì Arc scores: {result['arc_scores'].shape}")
                print(f"     ‚úì Relation scores: {result['relation_scores'].shape}")


def test_multimodal_vision_language():
    """Test enhanced multimodal vision-language capabilities."""
    print("\nüñºÔ∏è Testing Enhanced Multimodal Vision-Language...")

    # Create configuration
    config = VisionLanguageConfig(
        vision_feature_dim=512,
        language_feature_dim=256,
        fusion_dim=384,
        vocab_size=1000,
        max_caption_length=20,
        num_classes=400  # For action recognition
    )

    # Test video-text-audio fusion task
    model = VisionLanguageModel(config, VisionLanguageTask.VIDEO_TEXT_AUDIO_FUSION)
    model.eval()

    # Test data
    batch_size = 2
    video_features = torch.randn(batch_size, 16, 512)  # Video sequence
    text_tokens = torch.randint(0, config.vocab_size, (batch_size, 20))
    audio_features = torch.randn(batch_size, 32, 256)

    print(f"   Video features: {video_features.shape}")
    print(f"   Text tokens: {text_tokens.shape}")
    print(f"   Audio features: {audio_features.shape}")

    with torch.no_grad():
        result = model(
            images=None,  # Not used for video task
            text_tokens=text_tokens,
            video_features=video_features,
            audio_features=audio_features
        )

        print(f"   ‚úì Multimodal logits: {result['task_output']['multimodal_logits'].shape}")
        print(f"   ‚úì Fused features: {result['task_output']['fused_features'].shape}")
        print(f"   ‚úì Video features: {result['video_features'].shape}")
        print(f"   ‚úì Audio features: {result['audio_features'].shape}")


def test_iot_edge_integration():
    """Test IoT sensors integration and edge computing."""
    print("\nüéØ Testing IoT Edge Integration...")

    # Create a simple model for edge deployment
    simple_model = torch.nn.Sequential(
        torch.nn.Linear(128, 64),
        torch.nn.ReLU(),
        torch.nn.Linear(64, 10)
    )

    # Test mobile deployment
    print("   Testing mobile deployment...")
    mobile_manager = create_mobile_deployment(simple_model)

    # Create sample sensor data
    sensor_data = [
        SensorData(
            sensor_id="temp_01",
            sensor_type=SensorType.TEMPERATURE,
            timestamp=time.time(),
            data=25.5,
            quality_score=0.95
        ),
        SensorData(
            sensor_id="accel_01",
            sensor_type=SensorType.ACCELEROMETER,
            timestamp=time.time(),
            data=[0.1, 0.2, 9.8],
            quality_score=0.92
        ),
        SensorData(
            sensor_id="gps_01",
            sensor_type=SensorType.GPS,
            timestamp=time.time(),
            data=[-122.4194, 37.7749],  # San Francisco
            quality_score=0.88
        )
    ]

    print(f"   Created {len(sensor_data)} sensor readings")

    # Start edge service
    mobile_manager.start_service()

    # Test sensor data processing
    sensor_json = {
        "sensors": [
            {
                "sensor_id": data.sensor_id,
                "sensor_type": data.sensor_type.value,
                "timestamp": data.timestamp,
                "data": data.data,
                "quality_score": data.quality_score
            }
            for data in sensor_data
        ]
    }

    # Process the data
    success = mobile_manager.inference_engine.add_sensor_data(sensor_data)
    print(f"   ‚úì Sensor data queued: {success}")

    # Wait for processing
    time.sleep(0.2)

    # Get result
    result = mobile_manager.inference_engine.get_result(timeout=1.0)
    if result:
        print(f"   ‚úì Processing result: {result['result'].shape}")
        print(f"   ‚úì Inference time: {result['inference_time_ms']:.2f}ms")

    # Get performance stats
    stats = mobile_manager.get_system_status()
    print(f"   ‚úì System status: {len(stats)} metrics")
    print(f"   ‚úì Device type: {stats['config']['device_type']}")
    print(f"   ‚úì Target latency: {stats['config']['target_latency_ms']}ms")

    # Test Raspberry Pi deployment
    print("\n   Testing Raspberry Pi deployment...")
    pi_manager = create_raspberry_pi_deployment(simple_model)
    pi_stats = pi_manager.get_system_status()
    print(f"   ‚úì Pi config: {pi_stats['config']['device_type']}")
    print(f"   ‚úì Pi memory limit: {pi_stats['config']['max_memory_mb']}MB")

    # Stop services
    mobile_manager.stop_service()


def main():
    """Run all Phase 3 feature tests."""
    print("="*80)
    print("üöÄ PHASE 3: ADVANCED INTELLIGENCE & MULTI-MODAL LEARNING TESTS")
    print("="*80)

    try:
        # Test advanced video processing
        test_advanced_video_processing()

        # Test video-text-audio fusion
        test_video_text_audio_fusion()

        # Test advanced language understanding
        test_advanced_language_understanding()

        # Test enhanced multimodal vision-language
        test_multimodal_vision_language()

        # Test IoT edge integration
        test_iot_edge_integration()

        print("\n" + "="*80)
        print("‚úÖ ALL PHASE 3 TESTS COMPLETED SUCCESSFULLY!")
        print("="*80)

        # Summary of implemented features
        print("\nüìã IMPLEMENTED FEATURES SUMMARY:")
        print("üé• Video Processing Enhancement:")
        print("   ‚úì Advanced temporal reasoning across video sequences")
        print("   ‚úì Advanced action recognition and prediction")
        print("   ‚úì Video-text-audio multimodal fusion")
        print("   ‚úì Enhanced video transformer capabilities")

        print("\nüó£Ô∏è Advanced Language Understanding:")
        print("   ‚úì Enhanced POS tagging with contextual embeddings")
        print("   ‚úì Advanced semantic role labeling and dependency parsing")
        print("   ‚úì Conversational AI capabilities")
        print("   ‚úì Domain-specific language adaptation")

        print("\nüéØ Real-World Application Integration:")
        print("   ‚úì IoT sensors and real-time data streams")
        print("   ‚úì Edge computing optimization")
        print("   ‚úì Mobile/embedded deployment capabilities")
        print("   ‚úì Production-ready API endpoints")

    except Exception as e:
        print(f"\n‚ùå Test failed with error: {e}")
        raise


if __name__ == "__main__":
    main()
